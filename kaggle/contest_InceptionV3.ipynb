{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Flatten\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from data_prepare import read_images_from_file, read_labels_from_file, H_IMAGE, W_IMAGE\n",
    "\n",
    "from random import sample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN_FILES = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.get_shape of <tf.Tensor 'concat_14:0' shape=(?, 3, 3, 2048) dtype=float32>>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(H_IMAGE, W_IMAGE, 3))\n",
    "base_model.output.get_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "predictions = Dense(4, activation='softmax')(x)\n",
    "\n",
    "model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.28133333  0.28133333  0.15066667  0.28666667]\n",
      " [ 0.37933333  0.37933333  0.072       0.16933333]\n",
      " [ 0.39933333  0.39933333  0.02933333  0.172     ]\n",
      " [ 0.34933333  0.34933333  0.12533333  0.176     ]\n",
      " [ 0.37866667  0.37866667  0.052       0.19066667]\n",
      " [ 0.26733333  0.26733333  0.164       0.30133333]\n",
      " [ 0.27933333  0.27933333  0.15066667  0.29066667]\n",
      " [ 0.30666667  0.30666667  0.12533333  0.26133333]\n",
      " [ 0.36066667  0.36066667  0.116       0.16266667]\n",
      " [ 0.29866667  0.29866667  0.14        0.26266667]\n",
      " [ 0.384       0.384       0.03        0.202     ]]\n"
     ]
    }
   ],
   "source": [
    "Y_train_pcnt = []\n",
    "\n",
    "for part_i in range(NUM_TRAIN_FILES):        \n",
    "    Y_train = read_labels_from_file('./data/train-labels.part{}.ubyte'.format(part_i))\n",
    "    Y_train = np.asarray([[(i == Y) for i in range(1, 5)] for Y in Y_train])\n",
    "    Y_train_pcnt.append(np.sum(Y_train, axis=0) / float(Y_train.shape[0]))\n",
    "\n",
    "print np.asarray(Y_train_pcnt, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "nb_epoch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 1/3 --------------------\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 279s - loss: 1.2266   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 262s - loss: 1.0440   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 276s - loss: 0.9651   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 263s - loss: 0.8507   \n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 179s - loss: 0.8399   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 262s - loss: 0.8874   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 270s - loss: 0.8318   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 260s - loss: 0.8414   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 267s - loss: 0.7505   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 260s - loss: 0.9515   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 269s - loss: 0.8762   \n",
      "-------------------- Epoch 2/3 --------------------\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 261s - loss: 0.7958   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 269s - loss: 0.7677   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 259s - loss: 0.6736   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 271s - loss: 0.8458   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 280s - loss: 0.7754   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 282s - loss: 0.8118   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 283s - loss: 0.7717   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 276s - loss: 0.7968   \n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 189s - loss: 0.7249   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 275s - loss: 0.8891   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 286s - loss: 0.7710   \n",
      "-------------------- Epoch 3/3 --------------------\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 278s - loss: 0.6481   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 278s - loss: 0.7100   \n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 185s - loss: 0.6778   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 278s - loss: 0.8300   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 279s - loss: 0.7340   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 288s - loss: 0.6880   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 291s - loss: 0.7417   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 292s - loss: 0.7750   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 290s - loss: 0.7716   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 296s - loss: 0.8352   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 288s - loss: 0.7532   \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nb_epoch):\n",
    "    print \"-\" * 20 + \" Epoch {}/{} \".format(epoch+1, nb_epoch) + \"-\" * 20\n",
    "    \n",
    "    for part_i in sample(range(NUM_TRAIN_FILES), NUM_TRAIN_FILES):\n",
    "        X_train = read_images_from_file('./data/train-images.part{}.ubyte'.format(part_i))\n",
    "        \n",
    "        Y_train = read_labels_from_file('./data/train-labels.part{}.ubyte'.format(part_i))\n",
    "        Y_train = np.asarray([[(i == Y) for i in range(1, 5)] for Y in Y_train])\n",
    "        \n",
    "        model.fit(X_train, Y_train, nb_epoch=1, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_arch = open('./models/model_arch1.json', 'w')\n",
    "f_arch.write(model.to_json())\n",
    "model.save_weights('./models/model_weights1.h5')\n",
    "f_arch.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "nb_epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 1/2 --------------------\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 187s - loss: 0.6685 - acc: 0.7190   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 286s - loss: 0.6893 - acc: 0.7187   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 284s - loss: 0.6042 - acc: 0.7623   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 286s - loss: 0.8054 - acc: 0.6617   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 291s - loss: 0.7597 - acc: 0.6743   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 288s - loss: 0.7208 - acc: 0.6923   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 280s - loss: 0.7231 - acc: 0.7103   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 293s - loss: 0.7253 - acc: 0.6963   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 282s - loss: 0.6400 - acc: 0.7480   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 287s - loss: 0.6813 - acc: 0.7233   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 279s - loss: 0.8262 - acc: 0.6590   \n",
      "-------------------- Epoch 2/2 --------------------\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 293s - loss: 0.6051 - acc: 0.7733   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 292s - loss: 0.7169 - acc: 0.7020   \n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 195s - loss: 0.6621 - acc: 0.7290   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 280s - loss: 0.8124 - acc: 0.6720   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 294s - loss: 0.6688 - acc: 0.7240   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 284s - loss: 0.6913 - acc: 0.7153   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 292s - loss: 0.6727 - acc: 0.7210   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 291s - loss: 0.6994 - acc: 0.7243   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 292s - loss: 0.6978 - acc: 0.7177   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 283s - loss: 0.6323 - acc: 0.7437   \n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 288s - loss: 0.6952 - acc: 0.7180   \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nb_epoch):\n",
    "    print \"-\" * 20 + \" Epoch {}/{} \".format(epoch+1, nb_epoch) + \"-\" * 20\n",
    "    \n",
    "    for part_i in sample(range(NUM_TRAIN_FILES), NUM_TRAIN_FILES):\n",
    "        X_train = read_images_from_file('./data/train-images.part{}.ubyte'.format(part_i))\n",
    "        \n",
    "        Y_train = read_labels_from_file('./data/train-labels.part{}.ubyte'.format(part_i))\n",
    "        Y_train = np.asarray([[(i == Y) for i in range(1, 5)] for Y in Y_train])\n",
    "        \n",
    "        model.fit(X_train, Y_train, nb_epoch=1, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_arch = open('./models/model2_arch.json', 'w')\n",
    "f_arch.write(model.to_json())\n",
    "model.save_weights('./models/model2_weights.h5')\n",
    "f_arch.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset_selective -f X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset_selective -f Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_TEST_FILES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 282s   \n",
      "3000/3000 [==============================] - 277s   \n",
      "3000/3000 [==============================] - 274s   \n",
      "3000/3000 [==============================] - 279s   \n",
      "1999/1999 [==============================] - 185s   \n"
     ]
    }
   ],
   "source": [
    "for part_i in range(NUM_TEST_FILES):\n",
    "    X_test = read_images_from_file('./data/test-images.part{}.ubyte'.format(part_i))\n",
    "        \n",
    "    Y_test = model.predict(X_test, verbose=1, batch_size=50)\n",
    "    Y_test = np.argmax(Y_test, axis=1).tolist()\n",
    "    \n",
    "    Y_labels.extend(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./data/sample_submission4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission['label'] = Y_labels\n",
    "submission['label'] += 1\n",
    "submission.to_csv('./submission_1.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset_selective -f X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1\n",
      "1 convolution2d_1\n",
      "2 batchnormalization_1\n",
      "3 convolution2d_2\n",
      "4 batchnormalization_2\n",
      "5 convolution2d_3\n",
      "6 batchnormalization_3\n",
      "7 maxpooling2d_1\n",
      "8 convolution2d_4\n",
      "9 batchnormalization_4\n",
      "10 convolution2d_5\n",
      "11 batchnormalization_5\n",
      "12 maxpooling2d_2\n",
      "13 convolution2d_9\n",
      "14 batchnormalization_9\n",
      "15 convolution2d_7\n",
      "16 convolution2d_10\n",
      "17 batchnormalization_7\n",
      "18 batchnormalization_10\n",
      "19 averagepooling2d_1\n",
      "20 convolution2d_6\n",
      "21 convolution2d_8\n",
      "22 convolution2d_11\n",
      "23 convolution2d_12\n",
      "24 batchnormalization_6\n",
      "25 batchnormalization_8\n",
      "26 batchnormalization_11\n",
      "27 batchnormalization_12\n",
      "28 mixed0\n",
      "29 convolution2d_16\n",
      "30 batchnormalization_16\n",
      "31 convolution2d_14\n",
      "32 convolution2d_17\n",
      "33 batchnormalization_14\n",
      "34 batchnormalization_17\n",
      "35 averagepooling2d_2\n",
      "36 convolution2d_13\n",
      "37 convolution2d_15\n",
      "38 convolution2d_18\n",
      "39 convolution2d_19\n",
      "40 batchnormalization_13\n",
      "41 batchnormalization_15\n",
      "42 batchnormalization_18\n",
      "43 batchnormalization_19\n",
      "44 mixed1\n",
      "45 convolution2d_23\n",
      "46 batchnormalization_23\n",
      "47 convolution2d_21\n",
      "48 convolution2d_24\n",
      "49 batchnormalization_21\n",
      "50 batchnormalization_24\n",
      "51 averagepooling2d_3\n",
      "52 convolution2d_20\n",
      "53 convolution2d_22\n",
      "54 convolution2d_25\n",
      "55 convolution2d_26\n",
      "56 batchnormalization_20\n",
      "57 batchnormalization_22\n",
      "58 batchnormalization_25\n",
      "59 batchnormalization_26\n",
      "60 mixed2\n",
      "61 convolution2d_28\n",
      "62 batchnormalization_28\n",
      "63 convolution2d_29\n",
      "64 batchnormalization_29\n",
      "65 convolution2d_27\n",
      "66 convolution2d_30\n",
      "67 batchnormalization_27\n",
      "68 batchnormalization_30\n",
      "69 maxpooling2d_3\n",
      "70 mixed3\n",
      "71 convolution2d_35\n",
      "72 batchnormalization_35\n",
      "73 convolution2d_36\n",
      "74 batchnormalization_36\n",
      "75 convolution2d_32\n",
      "76 convolution2d_37\n",
      "77 batchnormalization_32\n",
      "78 batchnormalization_37\n",
      "79 convolution2d_33\n",
      "80 convolution2d_38\n",
      "81 batchnormalization_33\n",
      "82 batchnormalization_38\n",
      "83 averagepooling2d_4\n",
      "84 convolution2d_31\n",
      "85 convolution2d_34\n",
      "86 convolution2d_39\n",
      "87 convolution2d_40\n",
      "88 batchnormalization_31\n",
      "89 batchnormalization_34\n",
      "90 batchnormalization_39\n",
      "91 batchnormalization_40\n",
      "92 mixed4\n",
      "93 convolution2d_45\n",
      "94 batchnormalization_45\n",
      "95 convolution2d_46\n",
      "96 batchnormalization_46\n",
      "97 convolution2d_42\n",
      "98 convolution2d_47\n",
      "99 batchnormalization_42\n",
      "100 batchnormalization_47\n",
      "101 convolution2d_43\n",
      "102 convolution2d_48\n",
      "103 batchnormalization_43\n",
      "104 batchnormalization_48\n",
      "105 averagepooling2d_5\n",
      "106 convolution2d_41\n",
      "107 convolution2d_44\n",
      "108 convolution2d_49\n",
      "109 convolution2d_50\n",
      "110 batchnormalization_41\n",
      "111 batchnormalization_44\n",
      "112 batchnormalization_49\n",
      "113 batchnormalization_50\n",
      "114 mixed5\n",
      "115 convolution2d_55\n",
      "116 batchnormalization_55\n",
      "117 convolution2d_56\n",
      "118 batchnormalization_56\n",
      "119 convolution2d_52\n",
      "120 convolution2d_57\n",
      "121 batchnormalization_52\n",
      "122 batchnormalization_57\n",
      "123 convolution2d_53\n",
      "124 convolution2d_58\n",
      "125 batchnormalization_53\n",
      "126 batchnormalization_58\n",
      "127 averagepooling2d_6\n",
      "128 convolution2d_51\n",
      "129 convolution2d_54\n",
      "130 convolution2d_59\n",
      "131 convolution2d_60\n",
      "132 batchnormalization_51\n",
      "133 batchnormalization_54\n",
      "134 batchnormalization_59\n",
      "135 batchnormalization_60\n",
      "136 mixed6\n",
      "137 convolution2d_65\n",
      "138 batchnormalization_65\n",
      "139 convolution2d_66\n",
      "140 batchnormalization_66\n",
      "141 convolution2d_62\n",
      "142 convolution2d_67\n",
      "143 batchnormalization_62\n",
      "144 batchnormalization_67\n",
      "145 convolution2d_63\n",
      "146 convolution2d_68\n",
      "147 batchnormalization_63\n",
      "148 batchnormalization_68\n",
      "149 averagepooling2d_7\n",
      "150 convolution2d_61\n",
      "151 convolution2d_64\n",
      "152 convolution2d_69\n",
      "153 convolution2d_70\n",
      "154 batchnormalization_61\n",
      "155 batchnormalization_64\n",
      "156 batchnormalization_69\n",
      "157 batchnormalization_70\n",
      "158 mixed7\n",
      "159 convolution2d_73\n",
      "160 batchnormalization_73\n",
      "161 convolution2d_74\n",
      "162 batchnormalization_74\n",
      "163 convolution2d_71\n",
      "164 convolution2d_75\n",
      "165 batchnormalization_71\n",
      "166 batchnormalization_75\n",
      "167 convolution2d_72\n",
      "168 convolution2d_76\n",
      "169 batchnormalization_72\n",
      "170 batchnormalization_76\n",
      "171 averagepooling2d_8\n",
      "172 mixed8\n",
      "173 convolution2d_81\n",
      "174 batchnormalization_81\n",
      "175 convolution2d_78\n",
      "176 convolution2d_82\n",
      "177 batchnormalization_78\n",
      "178 batchnormalization_82\n",
      "179 convolution2d_79\n",
      "180 convolution2d_80\n",
      "181 convolution2d_83\n",
      "182 convolution2d_84\n",
      "183 averagepooling2d_9\n",
      "184 convolution2d_77\n",
      "185 batchnormalization_79\n",
      "186 batchnormalization_80\n",
      "187 batchnormalization_83\n",
      "188 batchnormalization_84\n",
      "189 convolution2d_85\n",
      "190 batchnormalization_77\n",
      "191 mixed9_0\n",
      "192 merge_1\n",
      "193 batchnormalization_85\n",
      "194 mixed9\n",
      "195 convolution2d_90\n",
      "196 batchnormalization_90\n",
      "197 convolution2d_87\n",
      "198 convolution2d_91\n",
      "199 batchnormalization_87\n",
      "200 batchnormalization_91\n",
      "201 convolution2d_88\n",
      "202 convolution2d_89\n",
      "203 convolution2d_92\n",
      "204 convolution2d_93\n",
      "205 averagepooling2d_10\n",
      "206 convolution2d_86\n",
      "207 batchnormalization_88\n",
      "208 batchnormalization_89\n",
      "209 batchnormalization_92\n",
      "210 batchnormalization_93\n",
      "211 convolution2d_94\n",
      "212 batchnormalization_86\n",
      "213 mixed9_1\n",
      "214 merge_2\n",
      "215 batchnormalization_94\n",
      "216 mixed10\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(base_model.layers):\n",
    "    print i, layer.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for layer in model.layers[:172]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in model.layers[172:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 1/10 --------------------\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 363s - loss: 0.6696 - acc: 0.7258 - val_loss: 0.9780 - val_acc: 0.8032\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 384s - loss: 0.7798 - acc: 0.6839 - val_loss: 0.6049 - val_acc: 0.7660\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 350s - loss: 0.6794 - acc: 0.7219 - val_loss: 0.5426 - val_acc: 0.8085\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 381s - loss: 0.6075 - acc: 0.7521 - val_loss: 0.5209 - val_acc: 0.7553\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 349s - loss: 0.6453 - acc: 0.7344 - val_loss: 0.5325 - val_acc: 0.7979\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 364s - loss: 0.6000 - acc: 0.7432 - val_loss: 0.6539 - val_acc: 0.7660\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 348s - loss: 0.5974 - acc: 0.7671 - val_loss: 0.9772 - val_acc: 0.7234\n",
      "Train on 1875 samples, validate on 125 samples\n",
      "Epoch 1/1\n",
      "1875/1875 [==============================] - 232s - loss: 0.6057 - acc: 0.7499 - val_loss: 0.5663 - val_acc: 0.7600\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 349s - loss: 0.6005 - acc: 0.7628 - val_loss: 0.9957 - val_acc: 0.7181\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 353s - loss: 0.5603 - acc: 0.7788 - val_loss: 0.7272 - val_acc: 0.8138\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 352s - loss: 0.6362 - acc: 0.7297 - val_loss: 0.3726 - val_acc: 0.8564\n",
      "-------------------- Epoch 2/10 --------------------\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 354s - loss: 0.5111 - acc: 0.8044 - val_loss: 0.9419 - val_acc: 0.7287\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 358s - loss: 0.5544 - acc: 0.7713 - val_loss: 0.6651 - val_acc: 0.7606\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 363s - loss: 0.5939 - acc: 0.7617 - val_loss: 0.5811 - val_acc: 0.8032\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 354s - loss: 0.5049 - acc: 0.7991 - val_loss: 0.8837 - val_acc: 0.8085\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 366s - loss: 0.5138 - acc: 0.7962 - val_loss: 0.4863 - val_acc: 0.7926\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 354s - loss: 0.5445 - acc: 0.7870 - val_loss: 0.9436 - val_acc: 0.7979\n",
      "Train on 1875 samples, validate on 125 samples\n",
      "Epoch 1/1\n",
      "1875/1875 [==============================] - 243s - loss: 0.5125 - acc: 0.7888 - val_loss: 0.5052 - val_acc: 0.8000\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 358s - loss: 0.7078 - acc: 0.7212 - val_loss: 0.5381 - val_acc: 0.7926\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 361s - loss: 0.5437 - acc: 0.7767 - val_loss: 0.3422 - val_acc: 0.8777\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 364s - loss: 0.5469 - acc: 0.7799 - val_loss: 0.4990 - val_acc: 0.8085\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 347s - loss: 0.5402 - acc: 0.7831 - val_loss: 0.8933 - val_acc: 0.7340\n",
      "-------------------- Epoch 3/10 --------------------\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 348s - loss: 0.4444 - acc: 0.8225 - val_loss: 0.4396 - val_acc: 0.8191\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 348s - loss: 0.4821 - acc: 0.8083 - val_loss: 0.6897 - val_acc: 0.8245\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 348s - loss: 0.4534 - acc: 0.8314 - val_loss: 0.8682 - val_acc: 0.7500\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 348s - loss: 0.6269 - acc: 0.7482 - val_loss: 0.5535 - val_acc: 0.7766\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 348s - loss: 0.4956 - acc: 0.7923 - val_loss: 0.3319 - val_acc: 0.8830\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 348s - loss: 0.5442 - acc: 0.7795 - val_loss: 0.5350 - val_acc: 0.8191\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 348s - loss: 0.4849 - acc: 0.8065 - val_loss: 0.4812 - val_acc: 0.8138\n",
      "Train on 1875 samples, validate on 125 samples\n",
      "Epoch 1/1\n",
      "1875/1875 [==============================] - 232s - loss: 0.4658 - acc: 0.8101 - val_loss: 0.5430 - val_acc: 0.7920\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 348s - loss: 0.4669 - acc: 0.8161 - val_loss: 0.8272 - val_acc: 0.7553\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 348s - loss: 0.4287 - acc: 0.8364 - val_loss: 0.7711 - val_acc: 0.8085\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 347s - loss: 0.5049 - acc: 0.7927 - val_loss: 0.6138 - val_acc: 0.7872\n",
      "-------------------- Epoch 4/10 --------------------\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 352s - loss: 0.3917 - acc: 0.8499 - val_loss: 0.7540 - val_acc: 0.8511\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 355s - loss: 0.4359 - acc: 0.8311 - val_loss: 1.0017 - val_acc: 0.7128\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 349s - loss: 0.4498 - acc: 0.8211 - val_loss: 0.5194 - val_acc: 0.7872\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 353s - loss: 0.4206 - acc: 0.8385 - val_loss: 0.8370 - val_acc: 0.7500\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 349s - loss: 0.4340 - acc: 0.8353 - val_loss: 0.7282 - val_acc: 0.8245\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 352s - loss: 0.4389 - acc: 0.8201 - val_loss: 0.3224 - val_acc: 0.8564\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 349s - loss: 0.4463 - acc: 0.8140 - val_loss: 0.4777 - val_acc: 0.8138\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 353s - loss: 0.4107 - acc: 0.8403 - val_loss: 0.4130 - val_acc: 0.8191\n",
      "Train on 1875 samples, validate on 125 samples\n",
      "Epoch 1/1\n",
      "1875/1875 [==============================] - 233s - loss: 0.3936 - acc: 0.8421 - val_loss: 0.5203 - val_acc: 0.8000\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 352s - loss: 0.4940 - acc: 0.8026 - val_loss: 0.4064 - val_acc: 0.8511\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 351s - loss: 0.5511 - acc: 0.7752 - val_loss: 0.5310 - val_acc: 0.7872\n",
      "-------------------- Epoch 5/10 --------------------\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 351s - loss: 0.3776 - acc: 0.8478 - val_loss: 0.5664 - val_acc: 0.7979\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 353s - loss: 0.4720 - acc: 0.8179 - val_loss: 0.5000 - val_acc: 0.7926\n",
      "Train on 1875 samples, validate on 125 samples\n",
      "Epoch 1/1\n",
      "1875/1875 [==============================] - 236s - loss: 0.3861 - acc: 0.8427 - val_loss: 0.4995 - val_acc: 0.8000\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 362s - loss: 0.3701 - acc: 0.8620 - val_loss: 0.8156 - val_acc: 0.7713\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 357s - loss: 0.3507 - acc: 0.8663 - val_loss: 0.7529 - val_acc: 0.8404\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 369s - loss: 0.3953 - acc: 0.8382 - val_loss: 0.2879 - val_acc: 0.8777\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 363s - loss: 0.3794 - acc: 0.8538 - val_loss: 0.9818 - val_acc: 0.7553\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 371s - loss: 0.4071 - acc: 0.8336 - val_loss: 0.4947 - val_acc: 0.8351\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 358s - loss: 0.3615 - acc: 0.8592 - val_loss: 0.3922 - val_acc: 0.8457\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 373s - loss: 0.3980 - acc: 0.8485 - val_loss: 0.7623 - val_acc: 0.8191\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 358s - loss: 0.4280 - acc: 0.8282 - val_loss: 0.4056 - val_acc: 0.8351\n",
      "-------------------- Epoch 6/10 --------------------\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 373s - loss: 0.3362 - acc: 0.8688 - val_loss: 0.5807 - val_acc: 0.7872\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 358s - loss: 0.3470 - acc: 0.8762 - val_loss: 0.8366 - val_acc: 0.7606\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 371s - loss: 0.3069 - acc: 0.8809 - val_loss: 0.3870 - val_acc: 0.8191\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 358s - loss: 0.3304 - acc: 0.8787 - val_loss: 0.9557 - val_acc: 0.7553\n",
      "Train on 1875 samples, validate on 125 samples\n",
      "Epoch 1/1\n",
      "1875/1875 [==============================] - 247s - loss: 0.3010 - acc: 0.8837 - val_loss: 0.4898 - val_acc: 0.8400\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 369s - loss: 0.3993 - acc: 0.8439 - val_loss: 0.4007 - val_acc: 0.8351\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 365s - loss: 0.3167 - acc: 0.8738 - val_loss: 0.6800 - val_acc: 0.8351\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 364s - loss: 0.3504 - acc: 0.8617 - val_loss: 0.2855 - val_acc: 0.8670\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 347s - loss: 0.3600 - acc: 0.8574 - val_loss: 0.6946 - val_acc: 0.8085\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 352s - loss: 0.4444 - acc: 0.8300 - val_loss: 0.4960 - val_acc: 0.7713\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 348s - loss: 0.3632 - acc: 0.8546 - val_loss: 0.4548 - val_acc: 0.8138\n",
      "-------------------- Epoch 7/10 --------------------\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 352s - loss: 0.3040 - acc: 0.8834 - val_loss: 0.4012 - val_acc: 0.8457\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 348s - loss: 0.3216 - acc: 0.8844 - val_loss: 0.4609 - val_acc: 0.8032\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 353s - loss: 0.3991 - acc: 0.8432 - val_loss: 0.5165 - val_acc: 0.7926\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 348s - loss: 0.2968 - acc: 0.8826 - val_loss: 0.6075 - val_acc: 0.8617\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 353s - loss: 0.3128 - acc: 0.8816 - val_loss: 0.8296 - val_acc: 0.7872\n",
      "Train on 1875 samples, validate on 125 samples\n",
      "Epoch 1/1\n",
      "1875/1875 [==============================] - 232s - loss: 0.2962 - acc: 0.8928 - val_loss: 0.4396 - val_acc: 0.8560\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 350s - loss: 0.3477 - acc: 0.8677 - val_loss: 0.5068 - val_acc: 0.8245\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 348s - loss: 0.3131 - acc: 0.8762 - val_loss: 0.2809 - val_acc: 0.8830\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 351s - loss: 0.2996 - acc: 0.8915 - val_loss: 0.7718 - val_acc: 0.7447\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 349s - loss: 0.3491 - acc: 0.8585 - val_loss: 0.6730 - val_acc: 0.7926\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 354s - loss: 0.3189 - acc: 0.8809 - val_loss: 0.9046 - val_acc: 0.8032\n",
      "-------------------- Epoch 8/10 --------------------\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 350s - loss: 0.2497 - acc: 0.9157 - val_loss: 0.9217 - val_acc: 0.8191\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 354s - loss: 0.2989 - acc: 0.8862 - val_loss: 0.4207 - val_acc: 0.8298\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 350s - loss: 0.3515 - acc: 0.8585 - val_loss: 0.5514 - val_acc: 0.7713\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 354s - loss: 0.2953 - acc: 0.8869 - val_loss: 0.4438 - val_acc: 0.7979\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 350s - loss: 0.2760 - acc: 0.8994 - val_loss: 0.2958 - val_acc: 0.8883\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 354s - loss: 0.2570 - acc: 0.9050 - val_loss: 0.7849 - val_acc: 0.8564\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 350s - loss: 0.2687 - acc: 0.9015 - val_loss: 0.8609 - val_acc: 0.7447\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 351s - loss: 0.2491 - acc: 0.9043 - val_loss: 0.4060 - val_acc: 0.8245\n",
      "Train on 1875 samples, validate on 125 samples\n",
      "Epoch 1/1\n",
      "1875/1875 [==============================] - 235s - loss: 0.2659 - acc: 0.8981 - val_loss: 0.4730 - val_acc: 0.8320\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 352s - loss: 0.2930 - acc: 0.8791 - val_loss: 0.5179 - val_acc: 0.7926\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 357s - loss: 0.2666 - acc: 0.9011 - val_loss: 0.9120 - val_acc: 0.7872\n",
      "-------------------- Epoch 9/10 --------------------\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 353s - loss: 0.2741 - acc: 0.8972 - val_loss: 0.4578 - val_acc: 0.8351\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 359s - loss: 0.2226 - acc: 0.9179 - val_loss: 0.7037 - val_acc: 0.8511\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 354s - loss: 0.2528 - acc: 0.9100 - val_loss: 0.5265 - val_acc: 0.7872\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 359s - loss: 0.2331 - acc: 0.9179 - val_loss: 0.9558 - val_acc: 0.7766\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 352s - loss: 0.2338 - acc: 0.9193 - val_loss: 1.1314 - val_acc: 0.7287\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 372s - loss: 0.2463 - acc: 0.9093 - val_loss: 0.9360 - val_acc: 0.8138\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 352s - loss: 0.2706 - acc: 0.8972 - val_loss: 0.5030 - val_acc: 0.8032\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 371s - loss: 0.2385 - acc: 0.9139 - val_loss: 0.4160 - val_acc: 0.8351\n",
      "Train on 1875 samples, validate on 125 samples\n",
      "Epoch 1/1\n",
      "1875/1875 [==============================] - 234s - loss: 0.2156 - acc: 0.9237 - val_loss: 0.4920 - val_acc: 0.8320\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 373s - loss: 0.3482 - acc: 0.8627 - val_loss: 0.5232 - val_acc: 0.7713\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 353s - loss: 0.2478 - acc: 0.9061 - val_loss: 0.2959 - val_acc: 0.8936\n",
      "-------------------- Epoch 10/10 --------------------\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 366s - loss: 0.2379 - acc: 0.9196 - val_loss: 0.3926 - val_acc: 0.8457\n",
      "Train on 1875 samples, validate on 125 samples\n",
      "Epoch 1/1\n",
      "1875/1875 [==============================] - 229s - loss: 0.2196 - acc: 0.9216 - val_loss: 0.4551 - val_acc: 0.8480\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 351s - loss: 0.2077 - acc: 0.9250 - val_loss: 0.2999 - val_acc: 0.8723\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 342s - loss: 0.2061 - acc: 0.9310 - val_loss: 1.0617 - val_acc: 0.7500\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 353s - loss: 0.2227 - acc: 0.9239 - val_loss: 0.9443 - val_acc: 0.7872\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 343s - loss: 0.1993 - acc: 0.9289 - val_loss: 0.4038 - val_acc: 0.8298\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 354s - loss: 0.2304 - acc: 0.9196 - val_loss: 0.5134 - val_acc: 0.8032\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 343s - loss: 0.2121 - acc: 0.9186 - val_loss: 0.8095 - val_acc: 0.8404\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 360s - loss: 0.2870 - acc: 0.8922 - val_loss: 0.5394 - val_acc: 0.7447\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 348s - loss: 0.2050 - acc: 0.9267 - val_loss: 1.0577 - val_acc: 0.8032\n",
      "Train on 2812 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "2812/2812 [==============================] - 363s - loss: 0.2226 - acc: 0.9154 - val_loss: 0.5071 - val_acc: 0.8298\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nb_epoch):\n",
    "    print \"-\" * 20 + \" Epoch {}/{} \".format(epoch+1, nb_epoch) + \"-\" * 20\n",
    "    \n",
    "    for part_i in sample(range(NUM_TRAIN_FILES), NUM_TRAIN_FILES):\n",
    "        X_train = read_images_from_file('./data/train-images.part{}.ubyte'.format(part_i))\n",
    "        \n",
    "        Y_train = read_labels_from_file('./data/train-labels.part{}.ubyte'.format(part_i))\n",
    "        Y_train = np.asarray([[(i == Y) for i in range(1, 5)] for Y in Y_train])\n",
    "        \n",
    "        model.fit(X_train, Y_train, nb_epoch=1, validation_split=0.0625, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_arch = open('./models/model3_arch.json', 'w')\n",
    "f_arch.write(model.to_json())\n",
    "model.save_weights('./models/model3_weights.h5')\n",
    "f_arch.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset_selective -f X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset_selective -f Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_TEST_FILES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 267s   \n",
      "3000/3000 [==============================] - 264s   \n",
      "3000/3000 [==============================] - 261s   \n",
      "3000/3000 [==============================] - 259s   \n",
      "1999/1999 [==============================] - 172s   \n"
     ]
    }
   ],
   "source": [
    "for part_i in range(NUM_TEST_FILES):\n",
    "    X_test = read_images_from_file('./data/test-images.part{}.ubyte'.format(part_i))\n",
    "        \n",
    "    Y_test = model.predict(X_test, verbose=1, batch_size=50)\n",
    "    Y_test = np.argmax(Y_test, axis=1).tolist()\n",
    "    \n",
    "    Y_labels.extend(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./data/sample_submission4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission['label'] = Y_labels\n",
    "submission['label'] += 1\n",
    "submission.to_csv('./submission_2.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset_selective -f X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
